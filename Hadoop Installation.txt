Pre-Requisites:
------------------------
1. Linux Operating System (Ubuntu)
2. Java
3. ssh
4. rsync

=====================================================
Installing java:
cmd: sudo apt-get install java-package

Installing ssh:
cmd: sudo apt-get install ssh
cmd: service ssh start

creating passwordlessssh:
cmd: ssh-keygen -t rsa
it will to enter password, here we have to press enter without entering our password:

Testing our ssh:
cmd: ssh username@hostname

To get hostname:
cmd: hostname

-----------------------------------------------
steps for downlaoding and installing Hadoop
---------------------------------------------
Download Apache Hadoop from hadoop.apache.org
preferably hadoop-1.x.x.tar

create bigdata under your home directory /home/username
cmd: mkdir bigdata
cmd: cd bigdata

Downloading the Hadoop tar file apache site:
cmd: wget http://apache.techartifact.com/mirror/hadoop/common/hadoop-1.0.4/hadoop-1.0.4.tar.gz

extract the tar file to hadoop-1.x.x directory
cmd: tar -xvf hadoop-1.x.x.tar

Change the directory permissions recursively to 755
755 means owner has full permissions, group and rest of the world have only read and execute permissions

cmd: chmod -R 755 hadoop-1.x.x
======================================================================
Installation Modes:
-------------------
1. Local mode
2. Pseudo Distributed Mode
3. Distributed Mode
-----------------------
We are having applying MapReduce on our Local file system:

In hadoop.1.x.x directory, we one sub directory called conf
In conf directory, we have files:

1. hadoop-env.sh
2. core-site.xml
3. hdfs-site.xml
4. mapred-site.xml
5. masters
6. slaves

hadoop-env.sh --> For setting, hadoop environment variables
core-site.xml --> For setting, Hadoop cluster Information related configuration parameters
hdfs-site.xml --> For setting, HDFS related configuration parameters
mapered-site.xml --> For setting, Map Reduce related configuration parameters
slaves --> All domain names (IP info) of slave nodes (Data Node + Task Tracker)
masters --> Domain name of Secondary Name Node

Default content in these files are:

all the xml files are with empty configuration information
both masters and slaves have hostname as localhost

In hadoop-env.sh --> All the default environment variables are configured.

================================================================
For running hadoop in local mode, only we have to modify hadoop-env.sh
In hadoop-env.sh, we have to Set JAVA_HOME
uncomment the JAVA_HOME and replace java path with our Java home:

Location of java home:
/usr/lib/jvm/javapackage/


For running Hadoop in pseudo distributed mode, we have to modify hadoop-env.sh.
In hadoop-env.sh, we have to Set JAVA_HOME.

We have to modify the some inportant configuration parameters in core-site.xml, hdfs-site.xml, mapred-site.xml, slaves, masters

In core-site.xml:
<parameter><name>fs.default.name</name><value>hdfs://hostname:port</value></parameter>

In mapred-site.xml:
<parameter><name>mapred.job.tracker</name><value>hostname:port</value></parameter>

In hdfs-site.xml:
<parameter><name>dfs.replication</name><value>1</value></parameter>
<parameter><name>dfs.name.dir</name><value>path of namenode[namenode meta information directory] directory</value></parameter>
<parameter><name>dfs.data.dir</name><value>path of datanode[actual data location] directory</value></parameter>

slaves:
hostname

masters:
hostname

For running Hadoop in pseudo distributed mode, we have to modify hadoop-env.sh.
In hadoop-env.sh, we have to Set JAVA_HOME.

copy the entire hadoop-1.x.x directory to the same path in the slave machines like
in master /home/hadoop/bigdata/hadoop-1.x.x. The absolute path of hadoop-1.x.x is same on all machines.

no change to core-site.xml
no change to mapred-site.xml
for hdfs-site.xml aslo changes are not required. If we want more replication value, we can change the dfs.replication parameter.
<parameter><name>dfs.replication</name><value>replication factor</value></parameter>

masters file:
Master Node : Enter the secondary namenode machine hostname
Slave Nodes: Empty the file

slaves file
Master Node: Enter all the list of Slave Node machines hostnames
slave1
slave2
.
.
.

Slave Nodes: Empty the file

Please follow the above guide lines while installing in all modes.